#!/usr/bin/env python3
"""
HCM ì‹œìŠ¤í…œ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ìˆ˜í•™ì  ê²€ì¦ ë° ì‹œë®¬ë ˆì´ì…˜
Mathematical Verification and Simulation for HCM Matching Algorithm
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.metrics import mean_squared_error, accuracy_score
from dataclasses import dataclass
from typing import List, Dict, Tuple
import random
import json
from datetime import datetime, timedelta

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['Malgun Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

@dataclass
class Employee:
    id: str
    name: str
    skills: Dict[str, float]  # skill_name: proficiency (0-10)
    experience_years: float
    department: str
    availability: float  # 0-1
    workload: float  # current workload 0-1

@dataclass
class Task:
    id: str
    title: str
    required_skills: Dict[str, float]  # skill_name: required_level (0-10)
    estimated_hours: float
    priority: int  # 1-10
    deadline_days: int
    complexity: float  # 0-1

@dataclass
class MatchResult:
    employee_id: str
    task_id: str
    match_score: float
    confidence: float
    reasoning: Dict[str, float]

class HCMMatchingSimulator:
    """HCM ì‹œìŠ¤í…œ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ì‹œë®¬ë ˆì´í„°"""
    
    def __init__(self):
        self.employees: List[Employee] = []
        self.tasks: List[Task] = []
        self.match_results: List[MatchResult] = []
        
    def generate_sample_data(self, num_employees: int = 100, num_tasks: int = 50):
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        # ê¸°ìˆ  ëª©ë¡
        skills = ['JavaScript', 'Python', 'Java', 'React', 'Node.js', 'SQL', 
                 'Machine Learning', 'DevOps', 'UI/UX', 'Project Management']
        
        departments = ['Development', 'QA', 'DevOps', 'Design', 'Management']
        
        # ì§ì› ë°ì´í„° ìƒì„±
        for i in range(num_employees):
            employee_skills = {}
            # ê° ì§ì›ë§ˆë‹¤ 3-7ê°œ ìŠ¤í‚¬ì„ ëœë¤í•˜ê²Œ ì„ íƒ
            selected_skills = random.sample(skills, random.randint(3, 7))
            for skill in selected_skills:
                # ìŠ¤í‚¬ ìˆ™ë ¨ë„ëŠ” ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„ (í‰ê·  6, í‘œì¤€í¸ì°¨ 2)
                proficiency = max(1, min(10, np.random.normal(6, 2)))
                employee_skills[skill] = proficiency
            
            employee = Employee(
                id=f"EMP_{i:03d}",
                name=f"Employee_{i:03d}",
                skills=employee_skills,
                experience_years=max(0, np.random.exponential(5)),
                department=random.choice(departments),
                availability=random.uniform(0.3, 1.0),
                workload=random.uniform(0.0, 0.8)
            )
            self.employees.append(employee)
        
        # íƒœìŠ¤í¬ ë°ì´í„° ìƒì„±
        for i in range(num_tasks):
            required_skills = {}
            # ê° íƒœìŠ¤í¬ë§ˆë‹¤ 2-5ê°œ ìŠ¤í‚¬ ìš”êµ¬
            selected_skills = random.sample(skills, random.randint(2, 5))
            for skill in selected_skills:
                required_level = random.uniform(3, 9)
                required_skills[skill] = required_level
            
            task = Task(
                id=f"TASK_{i:03d}",
                title=f"Task_{i:03d}",
                required_skills=required_skills,
                estimated_hours=random.uniform(8, 120),
                priority=random.randint(1, 10),
                deadline_days=random.randint(1, 30),
                complexity=random.uniform(0.2, 1.0)
            )
            self.tasks.append(task)
    
    def calculate_skill_match_score(self, employee: Employee, task: Task) -> Tuple[float, Dict[str, float]]:
        """ìŠ¤í‚¬ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (ìˆ˜í•™ì  ëª¨ë¸)"""
        skill_scores = {}
        total_weight = 0
        weighted_score = 0
        
        for skill, required_level in task.required_skills.items():
            employee_level = employee.skills.get(skill, 0)
            
            # ê°€ì¤‘ì¹˜: ìš”êµ¬ ë ˆë²¨ì´ ë†’ì„ìˆ˜ë¡ ì¤‘ìš”
            weight = required_level / 10.0
            total_weight += weight
            
            # ìŠ¤í‚¬ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì‚¬ìš©)
            if employee_level == 0:
                skill_score = 0
            else:
                # ì§ì› ë ˆë²¨ì´ ìš”êµ¬ ë ˆë²¨ë³´ë‹¤ ë†’ìœ¼ë©´ ë³´ë„ˆìŠ¤, ë‚®ìœ¼ë©´ í˜ë„í‹°
                diff = employee_level - required_level
                skill_score = 1 / (1 + np.exp(-diff))  # ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜
            
            skill_scores[skill] = skill_score
            weighted_score += skill_score * weight
        
        if total_weight == 0:
            return 0, skill_scores
        
        final_score = weighted_score / total_weight
        return final_score, skill_scores
    
    def calculate_availability_score(self, employee: Employee, task: Task) -> float:
        """ê°€ìš©ì„± ì ìˆ˜ ê³„ì‚°"""
        # í˜„ì¬ ì›Œí¬ë¡œë“œì™€ íƒœìŠ¤í¬ ì˜ˆìƒ ì‹œê°„ì„ ê³ ë ¤
        available_capacity = employee.availability * (1 - employee.workload)
        required_capacity = min(1.0, task.estimated_hours / 160)  # ì›” 160ì‹œê°„ ê¸°ì¤€
        
        if available_capacity >= required_capacity:
            return 1.0
        else:
            return available_capacity / required_capacity
    
    def calculate_experience_score(self, employee: Employee, task: Task) -> float:
        """ê²½í—˜ ì ìˆ˜ ê³„ì‚°"""
        # ë³µì¡ë„ì— ë”°ë¥¸ í•„ìš” ê²½í—˜ë…„ìˆ˜ ê³„ì‚°
        required_experience = task.complexity * 10  # 0-10ë…„
        
        if employee.experience_years >= required_experience:
            return 1.0
        else:
            # ê²½í—˜ ë¶€ì¡± ì‹œ ì ì§„ì  ê°ì†Œ
            return employee.experience_years / required_experience
    
    def calculate_priority_urgency_score(self, task: Task) -> float:
        """ìš°ì„ ìˆœìœ„ ë° ê¸´ê¸‰ë„ ì ìˆ˜"""
        priority_score = task.priority / 10.0
        urgency_score = max(0, (30 - task.deadline_days) / 30.0)
        return (priority_score + urgency_score) / 2
    
    def calculate_match_score(self, employee: Employee, task: Task) -> MatchResult:
        """ì¢…í•© ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        # ê° ìš”ì†Œë³„ ì ìˆ˜ ê³„ì‚°
        skill_score, skill_breakdown = self.calculate_skill_match_score(employee, task)
        availability_score = self.calculate_availability_score(employee, task)
        experience_score = self.calculate_experience_score(employee, task)
        priority_score = self.calculate_priority_urgency_score(task)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weights = {
            'skill': 0.4,
            'availability': 0.25,
            'experience': 0.2,
            'priority': 0.15
        }
        
        final_score = (
            skill_score * weights['skill'] +
            availability_score * weights['availability'] +
            experience_score * weights['experience'] +
            priority_score * weights['priority']
        )
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ìŠ¤í‚¬ ì»¤ë²„ë¦¬ì§€ ê¸°ë°˜)
        skill_coverage = len([s for s in task.required_skills.keys() 
                            if s in employee.skills]) / len(task.required_skills)
        confidence = skill_coverage * min(1.0, skill_score + 0.5)
        
        reasoning = {
            'skill_score': skill_score,
            'availability_score': availability_score,
            'experience_score': experience_score,
            'priority_score': priority_score,
            'skill_coverage': skill_coverage
        }
        
        return MatchResult(
            employee_id=employee.id,
            task_id=task.id,
            match_score=final_score,
            confidence=confidence,
            reasoning=reasoning
        )
    
    def run_matching_simulation(self) -> pd.DataFrame:
        """ì „ì²´ ë§¤ì¹­ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰"""
        results = []
        
        for task in self.tasks:
            task_matches = []
            for employee in self.employees:
                match_result = self.calculate_match_score(employee, task)
                task_matches.append(match_result)
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            task_matches.sort(key=lambda x: x.match_score, reverse=True)
            
            # ìƒìœ„ 5ëª… ì €ì¥
            for i, match in enumerate(task_matches[:5]):
                results.append({
                    'task_id': task.id,
                    'employee_id': match.employee_id,
                    'rank': i + 1,
                    'match_score': match.match_score,
                    'confidence': match.confidence,
                    'skill_score': match.reasoning['skill_score'],
                    'availability_score': match.reasoning['availability_score'],
                    'experience_score': match.reasoning['experience_score'],
                    'priority_score': match.reasoning['priority_score'],
                    'task_priority': task.priority,
                    'task_complexity': task.complexity,
                    'task_estimated_hours': task.estimated_hours
                })
        
        return pd.DataFrame(results)
    
    def statistical_analysis(self, results_df: pd.DataFrame) -> Dict:
        """í†µê³„ì  ë¶„ì„"""
        stats_summary = {
            'match_score_stats': {
                'mean': results_df['match_score'].mean(),
                'std': results_df['match_score'].std(),
                'min': results_df['match_score'].min(),
                'max': results_df['match_score'].max(),
                'q25': results_df['match_score'].quantile(0.25),
                'q75': results_df['match_score'].quantile(0.75)
            },
            'confidence_stats': {
                'mean': results_df['confidence'].mean(),
                'std': results_df['confidence'].std(),
                'high_confidence_ratio': (results_df['confidence'] > 0.8).mean()
            },
            'correlation_analysis': {
                'skill_vs_match': results_df['skill_score'].corr(results_df['match_score']),
                'experience_vs_match': results_df['experience_score'].corr(results_df['match_score']),
                'availability_vs_match': results_df['availability_score'].corr(results_df['match_score'])
            }
        }
        
        return stats_summary

def run_mathematical_verification():
    """ìˆ˜í•™ì  ê²€ì¦ ì‹¤í–‰"""
    print("ğŸ”¬ HCM ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ìˆ˜í•™ì  ê²€ì¦ ì‹œì‘...")
    
    # ì‹œë®¬ë ˆì´í„° ì´ˆê¸°í™”
    simulator = HCMMatchingSimulator()
    
    # ë‹¤ì–‘í•œ ê·œëª¨ë¡œ í…ŒìŠ¤íŠ¸
    test_scenarios = [
        {'employees': 50, 'tasks': 25, 'name': 'Small Scale'},
        {'employees': 100, 'tasks': 50, 'name': 'Medium Scale'},
        {'employees': 200, 'tasks': 100, 'name': 'Large Scale'}
    ]
    
    all_results = []
    
    for scenario in test_scenarios:
        print(f"\nğŸ“Š {scenario['name']} ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰...")
        print(f"   ì§ì› ìˆ˜: {scenario['employees']}, íƒœìŠ¤í¬ ìˆ˜: {scenario['tasks']}")
        
        # ë°ì´í„° ìƒì„±
        simulator.generate_sample_data(scenario['employees'], scenario['tasks'])
        
        # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        results_df = simulator.run_matching_simulation()
        results_df['scenario'] = scenario['name']
        all_results.append(results_df)
        
        # í†µê³„ ë¶„ì„
        stats = simulator.statistical_analysis(results_df)
        
        print(f"   í‰ê·  ë§¤ì¹­ ì ìˆ˜: {stats['match_score_stats']['mean']:.3f}")
        print(f"   ê³ ì‹ ë¢°ë„ ë§¤ì¹­ ë¹„ìœ¨: {stats['confidence_stats']['high_confidence_ratio']:.1%}")
        print(f"   ìŠ¤í‚¬-ë§¤ì¹­ ìƒê´€ê´€ê³„: {stats['correlation_analysis']['skill_vs_match']:.3f}")
    
    # ì „ì²´ ê²°ê³¼ ë³‘í•©
    combined_results = pd.concat(all_results, ignore_index=True)
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"./test-results/matching_verification_{timestamp}.csv"
    combined_results.to_csv(results_file, index=False, encoding='utf-8-sig')
    
    print(f"\nâœ… ê²€ì¦ ì™„ë£Œ! ê²°ê³¼ ì €ì¥: {results_file}")
    
    return combined_results

def create_visualization_plots(results_df: pd.DataFrame):
    """ì‹œê°í™” ìƒì„±"""
    plt.style.use('default')
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('HCM ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¶„ì„', fontsize=16, fontweight='bold')
    
    # 1. ë§¤ì¹­ ì ìˆ˜ ë¶„í¬
    axes[0, 0].hist(results_df['match_score'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0, 0].set_title('ë§¤ì¹­ ì ìˆ˜ ë¶„í¬')
    axes[0, 0].set_xlabel('ë§¤ì¹­ ì ìˆ˜')
    axes[0, 0].set_ylabel('ë¹ˆë„')
    axes[0, 0].axvline(results_df['match_score'].mean(), color='red', linestyle='--', 
                      label=f'í‰ê· : {results_df["match_score"].mean():.3f}')
    axes[0, 0].legend()
    
    # 2. ì‹ ë¢°ë„ vs ë§¤ì¹­ ì ìˆ˜
    axes[0, 1].scatter(results_df['confidence'], results_df['match_score'], alpha=0.6, color='green')
    axes[0, 1].set_title('ì‹ ë¢°ë„ vs ë§¤ì¹­ ì ìˆ˜')
    axes[0, 1].set_xlabel('ì‹ ë¢°ë„')
    axes[0, 1].set_ylabel('ë§¤ì¹­ ì ìˆ˜')
    
    # íšŒê·€ì„  ì¶”ê°€
    z = np.polyfit(results_df['confidence'], results_df['match_score'], 1)
    p = np.poly1d(z)
    axes[0, 1].plot(results_df['confidence'], p(results_df['confidence']), "r--", alpha=0.8)
    
    # 3. íƒœìŠ¤í¬ ë³µì¡ë„ë³„ ë§¤ì¹­ ì„±ëŠ¥
    complexity_bins = pd.cut(results_df['task_complexity'], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
    complexity_scores = results_df.groupby(complexity_bins)['match_score'].mean()
    axes[0, 2].bar(range(len(complexity_scores)), complexity_scores.values, color='orange', alpha=0.7)
    axes[0, 2].set_title('íƒœìŠ¤í¬ ë³µì¡ë„ë³„ í‰ê·  ë§¤ì¹­ ì ìˆ˜')
    axes[0, 2].set_xlabel('ë³µì¡ë„')
    axes[0, 2].set_ylabel('í‰ê·  ë§¤ì¹­ ì ìˆ˜')
    axes[0, 2].set_xticks(range(len(complexity_scores)))
    axes[0, 2].set_xticklabels(complexity_scores.index, rotation=45)
    
    # 4. ìš°ì„ ìˆœìœ„ë³„ ì„±ëŠ¥
    priority_groups = results_df.groupby('task_priority')['match_score'].mean()
    axes[1, 0].plot(priority_groups.index, priority_groups.values, marker='o', linewidth=2, markersize=6, color='purple')
    axes[1, 0].set_title('íƒœìŠ¤í¬ ìš°ì„ ìˆœìœ„ë³„ í‰ê·  ë§¤ì¹­ ì ìˆ˜')
    axes[1, 0].set_xlabel('ìš°ì„ ìˆœìœ„')
    axes[1, 0].set_ylabel('í‰ê·  ë§¤ì¹­ ì ìˆ˜')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 5. ê° ìš”ì†Œë³„ ê¸°ì—¬ë„
    factors = ['skill_score', 'availability_score', 'experience_score', 'priority_score']
    factor_means = [results_df[factor].mean() for factor in factors]
    factor_labels = ['ìŠ¤í‚¬', 'ê°€ìš©ì„±', 'ê²½í—˜', 'ìš°ì„ ìˆœìœ„']
    
    axes[1, 1].bar(factor_labels, factor_means, color=['red', 'blue', 'green', 'orange'], alpha=0.7)
    axes[1, 1].set_title('ë§¤ì¹­ ìš”ì†Œë³„ í‰ê·  ì ìˆ˜')
    axes[1, 1].set_ylabel('í‰ê·  ì ìˆ˜')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    # 6. ìˆœìœ„ë³„ ì ìˆ˜ ë¶„í¬
    rank_scores = results_df.groupby('rank')['match_score'].mean()
    axes[1, 2].bar(rank_scores.index, rank_scores.values, color='teal', alpha=0.7)
    axes[1, 2].set_title('ìˆœìœ„ë³„ í‰ê·  ë§¤ì¹­ ì ìˆ˜')
    axes[1, 2].set_xlabel('ìˆœìœ„')
    axes[1, 2].set_ylabel('í‰ê·  ë§¤ì¹­ ì ìˆ˜')
    
    plt.tight_layout()
    
    # ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plot_file = f"./test-results/matching_analysis_{timestamp}.png"
    plt.savefig(plot_file, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“Š ì‹œê°í™” ì €ì¥: {plot_file}")

if __name__ == "__main__":
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    import os
    os.makedirs("./test-results", exist_ok=True)
    
    # ìˆ˜í•™ì  ê²€ì¦ ì‹¤í–‰
    results = run_mathematical_verification()
    
    # ì‹œê°í™” ìƒì„±
    create_visualization_plots(results)
    
    print("\nğŸ‰ ìˆ˜í•™ì  ê²€ì¦ ë° ì‹œê°í™” ì™„ë£Œ!")
    print("ğŸ“ ê²°ê³¼ íŒŒì¼ë“¤ì´ ./test-results/ ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
